# üîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ

## üî¥ –ö–†–ò–¢–ò–ß–ù–û (Priority 1) - –ò—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ

### 1. –ò—Å–ø—Ä–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É Consumption –¥–∞–Ω–Ω—ã—Ö

**–§–∞–π–ª:** `src/data_loader.py:219-238`

**–¢–µ–∫—É—â–∏–π –∫–æ–¥ (–ù–ï–í–ï–†–ù–û):**
```python
consumption_pivot = consumption_df.pivot_table(
    index='territory_id',
    columns='category',
    values='value',
    aggfunc='mean'  # ‚ùå –£–Ω–∏—á—Ç–æ–∂–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
).reset_index()
```

**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ (Option 1 - Latest period):**
```python
# –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
consumption_latest = consumption_df.sort_values('date').groupby(
    ['territory_id', 'category']
).last().reset_index()

consumption_pivot = consumption_latest.pivot(
    index='territory_id',
    columns='category',
    values='value'
).reset_index()
```

**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ (Option 2 - Keep temporal structure):**
```python
# –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è temporal analysis
consumption_pivot = consumption_df.pivot(
    index=['territory_id', 'date'],
    columns='category',
    values='value'
).reset_index()

# –ó–∞—Ç–µ–º –≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å groupby('territory_id') –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
```

**–≠—Ñ—Ñ–µ–∫—Ç:** -25% false positives, –≤–∫–ª—é—á–µ–Ω–∏–µ temporal detection

---

### 2. –û—Ç–∫–ª—é—á–∏—Ç—å CrossSourceComparator

**–§–∞–π–ª:** `config.yaml`

**–î–æ–±–∞–≤–∏—Ç—å:**
```yaml
detectors:
  statistical: 
    enabled: true
  temporal:
    enabled: true
  geographic:
    enabled: true
  cross_source:
    enabled: false  # ‚ùå –ù–µ—Ç –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –º–µ—Ç—Ä–∏–∫ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
  logical:
    enabled: true
```

**–ò–ª–∏ –≤ `src/detector_manager.py` –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å:**
```python
# detectors['cross_source'] = CrossSourceComparator(self.config)
```

**–≠—Ñ—Ñ–µ–∫—Ç:** -15% false positives (-2,414 –∞–Ω–æ–º–∞–ª–∏–π)

---

### 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π

**–§–∞–π–ª:** `src/data_loader.py`

**–î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥:**
```python
def load_connection_data(self) -> pd.DataFrame:
    """
    Load connection graph data.
    
    Returns:
        DataFrame with columns:
        - territory_id_x: Source territory
        - territory_id_y: Target territory
        - distance: Distance in km
        - type: Connection type (highway)
    """
    logger.info("Loading connection graph...")
    file_path = self.base_path / 'connection.parquet'
    
    try:
        df = pd.read_parquet(file_path)
        logger.info(f"Loaded connection graph: {df.shape[0]} connections")
        return df
    except FileNotFoundError:
        logger.warning(f"Connection file not found: {file_path}")
        return pd.DataFrame()
```

**–§–∞–π–ª:** `src/anomaly_detector.py` - GeographicAnomalyDetector

**–î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥:**
```python
def _get_neighbors(
    self, 
    territory_id: int, 
    connections: pd.DataFrame, 
    max_distance: float = 50.0
) -> List[int]:
    """
    Get neighboring territories from connection graph.
    
    Args:
        territory_id: Territory ID to find neighbors for
        connections: Connection graph DataFrame
        max_distance: Maximum distance in km to consider as neighbor
    
    Returns:
        List of neighbor territory IDs
    """
    # Find all connections involving this territory
    neighbors_x = connections[
        (connections['territory_id_x'] == territory_id) &
        (connections['distance'] <= max_distance)
    ]['territory_id_y'].tolist()
    
    neighbors_y = connections[
        (connections['territory_id_y'] == territory_id) &
        (connections['distance'] <= max_distance)
    ]['territory_id_x'].tolist()
    
    # Combine and deduplicate
    all_neighbors = list(set(neighbors_x + neighbors_y))
    
    return all_neighbors
```

**–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å detect_cluster_outliers:**
```python
def detect_cluster_outliers(
    self, 
    df: pd.DataFrame, 
    connections: pd.DataFrame  # ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä
) -> List[Dict[str, Any]]:
    """
    Detect municipalities that differ from their neighbors.
    Uses real connection graph instead of administrative regions.
    """
    anomalies = []
    
    if connections.empty:
        self.logger.warning("No connection data - falling back to region-based clustering")
        return self._detect_cluster_outliers_legacy(df)
    
    # ... (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º _get_neighbors)
```

**–≠—Ñ—Ñ–µ–∫—Ç:** -10% false positives, –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ geographic anomalies

---

## üü° –í–ê–ñ–ù–û (Priority 2) - –ò—Å–ø—Ä–∞–≤–∏—Ç—å –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è

### 4. –£–∂–µ—Å—Ç–æ—á–∏—Ç—å –ø–æ—Ä–æ–≥–∏ –¥–ª—è –†–æ—Å—Å–∏–∏

**–§–∞–π–ª:** `config.yaml`

**–¢–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
```yaml
thresholds:
  statistical:
    z_score: 3.0
    iqr_multiplier: 1.5
  geographic:
    regional_z_score: 3.5
    cluster_threshold: 4.0
```

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
```yaml
thresholds:
  statistical:
    z_score: 5.0          # 3.0 ‚Üí 5.0 (–†–æ—Å—Å–∏—è –∫—Ä–∞–π–Ω–µ –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω–∞)
    iqr_multiplier: 3.0   # 1.5 ‚Üí 3.0
    percentile_lower: 0.1  # 1 ‚Üí 0.1 (—Ç–æ–ª—å–∫–æ extreme outliers)
    percentile_upper: 99.9 # 99 ‚Üí 99.9
  
  geographic:
    regional_z_score: 6.0  # 3.5 ‚Üí 6.0 (—É—á–µ—Å—Ç—å –ú–æ—Å–∫–≤–∞ vs –ß—É–∫–æ—Ç–∫–∞)
    cluster_threshold: 5.0 # 4.0 ‚Üí 5.0
  
  cross_source:
    enabled: false  # –û—Ç–∫–ª—é—á–∏—Ç—å
  
  logical:
    check_negative_values: true
    check_impossible_ratios: false  # –û—Ç–∫–ª—é—á–∏—Ç—å (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ª–µ–≥–∏—Ç–∏–º–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤)
```

**–≠—Ñ—Ñ–µ–∫—Ç:** -20% false positives

---

### 5. –û—Ç–∫–ª—é—á–∏—Ç—å Auto-tuning

**–§–∞–π–ª:** `config.yaml`

```yaml
auto_tuning:
  enabled: false  # ‚ùå –†–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ—Ç–∏–≤ —Ü–µ–ª–µ–π
```

**–ü—Ä–∏—á–∏–Ω–∞:** Auto-tuning –°–ú–Ø–ì–ß–ê–ï–¢ –ø–æ—Ä–æ–≥–∏ –≤–º–µ—Å—Ç–æ —É–∂–µ—Å—Ç–æ—á–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫:
- –ù–µ—Ç ground truth dataset
- FPR —Ä–∞—Å—á—ë—Ç –Ω–µ–≤–µ—Ä–µ–Ω
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏

**–≠—Ñ—Ñ–µ–∫—Ç:** –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–æ—Å—Ç–∞ false positives

---

### 6. –ü—Ä–∏–º–µ–Ω–∏—Ç—å log-transform –¥–ª—è —Å–∫–æ—à–µ–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

**–§–∞–π–ª:** `src/anomaly_detector.py` - StatisticalOutlierDetector

**–î–æ–±–∞–≤–∏—Ç—å –ø–µ—Ä–µ–¥ —Ä–∞—Å—á—ë—Ç–æ–º z-scores:**
```python
def detect_zscore_outliers(self, df: pd.DataFrame, threshold: Optional[float] = None):
    ...
    for indicator in indicator_cols:
        values = df[indicator].dropna()
        
        # Check skewness
        skewness = values.skew()
        
        if abs(skewness) > 2.0:
            # Highly skewed - apply log transform
            self.logger.debug(f"Applying log transform to '{indicator}' (skewness={skewness:.2f})")
            values_transformed = np.log1p(values)  # log(1+x) to handle zeros
            mean_val = values_transformed.mean()
            std_val = values_transformed.std()
            z_scores = (values_transformed - mean_val) / std_val
        else:
            # Normal distribution - use original values
            mean_val = values.mean()
            std_val = values.std()
            z_scores = (values - mean_val) / std_val
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ power-law —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π (–Ω–∞—Å–µ–ª–µ–Ω–∏–µ, –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ)

---

### 7. –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å legitimate pattern filter

**–§–∞–π–ª:** `main.py`

**–î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å—Ç—Ä–æ–∫–∞ 800):**
```python
# –ü–æ—Å–ª–µ combined_anomalies = aggregator.combine_anomalies(all_anomalies)

# Apply legitimate pattern filter
if len(combined_anomalies) > 0:
    try:
        from src.legitimate_pattern_filter import LegitimatePatternFilter
        
        logger.info("Applying legitimate pattern filter...")
        pattern_filter = LegitimatePatternFilter(config)
        
        # Filter anomalies
        filtered_anomalies = pattern_filter.filter_anomalies(combined_anomalies)
        
        # Count reclassified
        legitimate_count = (filtered_anomalies['is_legitimate_pattern'] == True).sum()
        logger.info(f"Reclassified {legitimate_count} anomalies as legitimate patterns")
        
        # Remove or flag legitimate patterns
        combined_anomalies = filtered_anomalies[
            filtered_anomalies['is_legitimate_pattern'] == False
        ]
        
        logger.info(f"After filtering: {len(combined_anomalies)} anomalies remain")
        
    except Exception as e:
        logger.warning(f"Failed to apply pattern filter: {e}")
        # Continue without filtering
```

**–≠—Ñ—Ñ–µ–∫—Ç:** -8% false positives (–ª–µ–≥–∏—Ç–∏–º–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: —Ç—É—Ä–∏–∑–º, –¥–µ–ª–æ–≤—ã–µ —Ü–µ–Ω—Ç—Ä—ã)

---

## üü¢ –ñ–ï–õ–ê–¢–ï–õ–¨–ù–û (Priority 3) - –£–ª—É—á—à–µ–Ω–∏—è

### 8. –î–æ–±–∞–≤–∏—Ç—å municipality whitelist

**–§–∞–π–ª:** `config.yaml`

```yaml
whitelists:
  # Territories with known unique characteristics
  unique_municipalities:
    - territory_id: 42  # –ù–æ—Ä–∏–ª—å—Å–∫ - —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π —Å–µ–≤–µ—Ä
      reason: "Northernmost major city, extreme conditions"
    - territory_id: 123  # –ë–∏–ª–∏–±–∏–Ω—Å–∫–∏–π —Ä–∞–π–æ–Ω
      reason: "Remote Chukotka region, naturally different"
    - territory_id: 456  # –°–æ—á–∏
      reason: "Major tourist destination"
  
  # Automatically whitelist by category
  auto_whitelist:
    - type: "capital"  # All regional capitals
    - type: "tourism_zone"  # Tourist territories
    - type: "industrial"  # Major industrial centers
```

**–≠—Ñ—Ñ–µ–∫—Ç:** -5% false positives (–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏)

---

### 9. –£–ª—É—á—à–∏—Ç—å Detection Metrics

**–§–∞–π–ª:** `src/detector_manager.py`

**–î–æ–±–∞–≤–∏—Ç—å —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫:**
```python
def calculate_detection_metrics(self, anomalies_df: pd.DataFrame, df: pd.DataFrame):
    """
    Calculate detection quality metrics.
    
    Returns:
        - coverage: % territories with at least 1 anomaly
        - intensity: avg anomalies per flagged territory
        - diversity: distribution across anomaly types
        - concentration: % anomalies in top 10% territories
    """
    metrics = {}
    
    total_territories = df['territory_id'].nunique()
    flagged_territories = anomalies_df['territory_id'].nunique()
    
    metrics['coverage'] = flagged_territories / total_territories
    metrics['intensity'] = len(anomalies_df) / flagged_territories if flagged_territories > 0 else 0
    
    # Type diversity
    type_counts = anomalies_df['anomaly_type'].value_counts()
    metrics['diversity'] = len(type_counts) / 5  # 5 types total
    
    # Concentration (Gini-like)
    terr_counts = anomalies_df['territory_id'].value_counts()
    top10_pct = int(len(terr_counts) * 0.1)
    top10_anomalies = terr_counts.head(top10_pct).sum()
    metrics['concentration'] = top10_anomalies / len(anomalies_df)
    
    return metrics
```

**–≠—Ñ—Ñ–µ–∫—Ç:** –õ—É—á—à–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏

---

## üìã –ü–û–®–ê–ì–û–í–´–ô –ü–õ–ê–ù –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø

### –®–∞–≥ 1: Critical Fixes (1-2 —á–∞—Å–∞)

1. ‚úÖ –ò—Å–ø—Ä–∞–≤–∏—Ç—å `data_loader.py` - consumption aggregation
2. ‚úÖ –û—Ç–∫–ª—é—á–∏—Ç—å CrossSourceComparator –≤ config
3. ‚úÖ –û—Ç–∫–ª—é—á–∏—Ç—å Auto-tuning –≤ config
4. ‚úÖ –£–∂–µ—Å—Ç–æ—á–∏—Ç—å –ø–æ—Ä–æ–≥–∏ –≤ config

**–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç:**
```bash
python main.py
python analyze_anomalies.py
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** –ê–Ω–æ–º–∞–ª–∏–π 8,000-10,000 (~50% —Å–Ω–∏–∂–µ–Ω–∏–µ)

---

### –®–∞–≥ 2: Important Fixes (2-3 —á–∞—Å–∞)

5. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É connection graph
6. ‚úÖ –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å GeographicAnomalyDetector
7. ‚úÖ –ü—Ä–∏–º–µ–Ω–∏—Ç—å log-transform –≤ StatisticalOutlierDetector
8. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å legitimate pattern filter

**–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç:**
```bash
python main.py
python analyze_anomalies.py
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** –ê–Ω–æ–º–∞–ª–∏–π 3,000-4,000 (~75-80% —Å–Ω–∏–∂–µ–Ω–∏–µ –æ—Ç original)

---

### –®–∞–≥ 3: Optional Improvements (1-2 —á–∞—Å–∞)

9. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å whitelist
10. ‚úÖ –£–ª—É—á—à–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏
11. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å unit tests
12. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é

---

## üéØ –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

### –ü–æ—Å–ª–µ –í–°–ï–• –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:

| –ú–µ—Ç—Ä–∏–∫–∞ | –°–µ–π—á–∞—Å | –ü–æ—Å–ª–µ | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|--------|-------|-----------|
| **–í—Å–µ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π** | 16,682 | ~3,500 | ‚Üì79% |
| **–¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π –ø–æ–º–µ—á–µ–Ω–æ** | 103% (2,659) | ~30% (~770) | ‚Üì70% |
| **Geographic anomalies** | 32.7% | ~18% | ‚Üì45% |
| **Cross-source** | 14.5% | 0% | ‚Üì100% |
| **Logical** | 24.1% | ~12% | ‚Üì50% |
| **Statistical** | 28.8% | ~15% | ‚Üì48% |
| **Temporal** | 0% | ~5% | NEW |

### –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:

‚úÖ **–¢–æ—á–Ω–æ—Å—Ç—å:** –ê–Ω–æ–º–∞–ª–∏–∏ —Ä–µ–∞–ª—å–Ω–æ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ (–Ω–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è)  
‚úÖ **–ü–æ–ª–Ω–æ—Ç–∞:** Temporal anomalies —Ç–µ–ø–µ—Ä—å –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç—Å—è  
‚úÖ **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:** –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –¥–ª—è geographic analysis  
‚úÖ **–ü–æ–Ω—è—Ç–Ω–æ—Å—Ç—å:** –ú–µ–Ω—å—à–µ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π = –ª–µ–≥—á–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å

---

## üìù –ß–ï–ö–õ–ò–°–¢ –î–õ–Ø –ü–†–û–í–ï–†–ö–ò

–ü–æ—Å–ª–µ –≤—Å–µ—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø—Ä–æ–≤–µ—Ä–∏—Ç—å:

- [ ] Consumption –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç date –∫–æ–ª–æ–Ω–∫—É
- [ ] TemporalAnomalyDetector –Ω–∞—Ö–æ–¥–∏—Ç >0 –∞–Ω–æ–º–∞–ª–∏–π
- [ ] GeographicAnomalyDetector –∏—Å–ø–æ–ª—å–∑—É–µ—Ç connection graph
- [ ] CrossSourceComparator –æ—Ç–∫–ª—é—á–µ–Ω
- [ ] Auto-tuning –æ—Ç–∫–ª—é—á–µ–Ω
- [ ] –ü–æ—Ä–æ–≥–∏ —É–∂–µ—Å—Ç–æ—á–µ–Ω—ã (z_score >= 5.0)
- [ ] Log-transform –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è skewed distributions
- [ ] Legitimate pattern filter –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω
- [ ] –í—Å–µ–≥–æ –∞–Ω–æ–º–∞–ª–∏–π < 5,000
- [ ] –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π –ø–æ–º–µ—á–µ–Ω–æ < 35%
- [ ] –ï—Å—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—è "legitimate_pattern" –∏–ª–∏ "temporal_anomaly"

---

**–î–∞—Ç–∞:** 6 –Ω–æ—è–±—Ä—è 2025  
**–ê–≤—Ç–æ—Ä:** –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–π –∞—É–¥–∏—Ç–æ—Ä  
**–°—Ç–∞—Ç—É—Å:** ‚è≥ –ì–æ—Ç–æ–≤ –∫ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é
