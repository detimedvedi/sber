# Структура сырых данных и работа детектора аномалий

## 1. СЫРЫЕ ДАННЫЕ

### СберИндекс (3 файла Parquet)

**connection.parquet** - 4.7M строк
- Граф связей между территориями (дороги, ЖД)
- Колонки: `territory_id_x`, `territory_id_y`, `distance`, `type`
- Типы: `highway` (3.3M), `railway` (1.5M)
- Покрывает: 2,630 территорий

**consumption.parquet** - 303K строк
- Потребление по категориям за период
- Колонки: `date`, `territory_id`, `category`, `value`
- 6 категорий: Продовольствие, Маркетплейсы, Все категории и т.д.
- **ВРЕМЕННЫЕ ДАННЫЕ:** ~24-72 периода на территорию

**market_access.parquet** - 2.6K строк
- Доступность рынков
- Колонки: `territory_id`, `market_access`
- По одной записи на территорию

### Росстат (3 файла Parquet)

**population** - 700K строк (временные ряды)
**migration** - 106K строк (временные ряды)
**salary** - 370K строк (временные ряды, 21 отрасль)

### Муниципальный словарь (Excel)

**t_dict_municipal_districts.xlsx** - 3,101 муниципалитет
- Справочник территорий
- Ключевые поля: `territory_id`, `municipal_district_name_short`, `region_name`

---

## 2. ОСОБЕННОСТИ ДАННЫХ

### ⚠️ Критические особенности:

**1. ВРЕМЕННАЯ СТРУКТУРА**
- Consumption: 24-72 периода на территорию
- Росстат: множественные периоды
- **Результат:** 60,308 строк в объединенном датасете (не 3,101!)
- **Дубликаты territory_id - это НОРМА, не баг**

**2. РАЗРЕЖЕННОСТЬ**
- 88.8% заполненность данных
- 26 колонок с пропусками
- 2 колонки с >50% пропусков (экзотические отрасли)
- 286 муниципалитетов с <50% данных

**3. ГЕТЕРОГЕННОСТЬ**
- Москва vs село в Чукотке - разница в 1000x
- Северные территории: высокие цены = высокое потребление
- Туристические города: высокое потребление, малое население
- Транзитные узлы: покупки проезжающих

**4. ГРАФ СВЯЗЕЙ**
- 4.7M связей между территориями
- Используется для кластерного анализа
- Позволяет сравнивать соседей

---

## 3. КАК РАБОТАЕТ ДЕТЕКТОР

### Этап 1: Загрузка и объединение (DataLoader)

```
1. Загружает 7 файлов
2. Объединяет по territory_id
3. Pivot для consumption (категории → колонки)
4. Cross-join с временными данными
5. Результат: 60,308 строк × 37 колонок
```

**Ключевая логика:**
- Детектирует временную структуру: `is_temporal=True`
- Создает source mapping (откуда каждая колонка)
- Валидирует качество данных

### Этап 2: Детекция аномалий (4 детектора)

**StatisticalOutlierDetector** (1,837 аномалий)
- Z-score = 5.0 (очень консервативно)
- IQR multiplier = 3.0
- Percentile: 0.1-99.9
- **Работает с:** каждой строкой независимо

**TemporalAnomalyDetector** (2,341 аномалий)
- Группирует по territory_id
- Детектирует: спайки (200%), дропы (-80%), волатильность (3x)
- **Работает с:** временными рядами по территориям
- **Использует:** 24-72 периода на территорию

**GeographicAnomalyDetector** (3,036 аномалий)
- Regional z-score = 6.0 (сравнение внутри региона)
- Cluster threshold = 5.0 (сравнение с соседями по графу)
- Urban/rural classification
- **Работает с:** последним периодом для географии
- **Использует:** connection graph для соседей

**LogicalConsistencyChecker** (9,523 аномалий - ПРОБЛЕМА!)
- Негативные значения (0 найдено)
- Противоречивые индикаторы (7,141 - МНОГО ЛОЖНЫХ)
- Дубликаты territory_id (2,152 - **БАГ!** это временные данные)
- Паттерны пропусков (87)

### Этап 3: Агрегация (ResultsAggregator)

```
1. Объединяет результаты 4 детекторов
2. Дедупликация (25,122 → 24,628)
3. Legitimate Pattern Filter (-7,891)
4. Результат: 16,737 уникальных аномалий
```

**Фильтрация:**
- Whitelist территорий (68 городов)
- Легитимные паттерны (северные территории, столицы)
- Priority scoring (severity × frequency × data_source)

### Этап 4: Экспорт

- CSV: все аномалии
- Excel: по типам + executive summary
- Визуализации: 4 графика
- Документация: методология, примеры

---

## 4. ПРОБЛЕМЫ ТЕКУЩЕЙ РЕАЛИЗАЦИИ

### БАГ: Дубликаты territory_id
```python
# LogicalConsistencyChecker флагает:
duplicate_ids = df[df.duplicated(subset=['territory_id'], keep=False)]
# Результат: 2,152 "аномалий"

# НО DataLoader уже знает:
is_temporal = True  # Это временные данные!
```

**Решение:** Пропускать проверку дубликатов если `is_temporal=True`

### СПОРНАЯ ЛОГИКА: Противоречивые индикаторы
```python
# Флагает если:
consumption > 75-й перцентиль AND population < 25-й перцентиль

# Примеры "аномалий":
# - Певек (Чукотка): высокое потребление, малое население
# - Карагинский (Камчатка): высокое потребление, малое население
```

**Проблема:** Это легитимные паттерны для северных/туристических территорий

**Решение:** 
- Ослабить пороги (90/10 вместо 75/25)
- Или добавить в legitimate patterns
- Или учитывать контекст (северные территории)

---

## 5. КОРРЕКТНЫЕ РЕЗУЛЬТАТЫ

После исправления багов:

```
Статистические:  1,837 ✓
Временные:       2,341 ✓
Географические:  3,036 ✓
Логические:      ~2,000 (после фикса)
─────────────────────────
ИТОГО:          ~9,200 аномалий

Территорий с аномалиями: ~1,500-2,000 (50-65%)
```

Это более реалистичная оценка для данных с такой гетерогенностью.
